{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import sys\n",
    "sys.path.append('/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline')\n",
    "sys.path.append('/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline/scripts')\n",
    "\n",
    "\n",
    "from scripts.configuration import RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "confargs = {\n",
    "    \"ignore_qc\": False, \"mfethreshold\": 90, \"maxsize\": 200,\n",
    "    \"target\": \"Lactobacillus_curvatus\", \"nolist\": False, \"skip_tree\": False,\n",
    "    \"blastseqs\": 1000, \"mfold\": -3.0, \"mpprimer\": -3.5,\n",
    "    \"offline\": False,\n",
    "    \"path\": os.path.join(\"/\", \"home\", \"ags-bioinfo\", \"blastdb\", \"primerdesign\", \"test\"),\n",
    "    \"probe\": True, \"exception\": [], \"minsize\": 70, \"skip_download\": True,\n",
    "    \"customdb\": os.path.join(\"/\", \"home\", \"ags-bioinfo\", \"blastdb\", \"BLASTDB_nt\", \"ref_prok_rep_genomes\"),\n",
    "    \"assemblylevel\": [\"all\"], \"qc_gene\": [\"rRNA\", \"tuf\"],\n",
    "    \"virus\": False, \"genbank\": False, \"intermediate\": True,\n",
    "    \"nontargetlist\": [\"Lactobacillus sakei\"],\n",
    "    \"evalue\": 500, \"nuc_identity\": 0, \"runmode\": [\"species\"], \"strains\": []}\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "def config():\n",
    "    from basicfunctions import HelperFunctions as H\n",
    "    from scripts.configuration import CLIconf\n",
    "    args = AttrDict(confargs)\n",
    "    nontargetlist = H.create_non_target_list(args.target)\n",
    "    config = CLIconf(\n",
    "            args.minsize, args.maxsize, args.mpprimer, args.exception,\n",
    "            args.target, args.path, args.intermediate,\n",
    "            args.qc_gene, args.mfold, args.skip_download,\n",
    "            args.assemblylevel, nontargetlist,\n",
    "            args.skip_tree, args.nolist, args.offline,\n",
    "            args.ignore_qc, args.mfethreshold, args.customdb,\n",
    "            args.blastseqs, args.probe, args.virus, args.genbank,\n",
    "            args.evalue, args.nuc_identity, args.runmode, args.strains)\n",
    "\n",
    "    config.save_config()\n",
    "\n",
    "    return config\n",
    "\n",
    "class RunConfig():\n",
    "    def __init__(self, configuration):\n",
    "        self.config = configuration\n",
    "        self.target = configuration.target\n",
    "        self.target_dir = os.path.join(self.config.path, self.target)\n",
    "        self.config_dir = os.path.join(self.target_dir, \"config\")\n",
    "        self.genomedata_dir = os.path.join(self.target_dir, \"genomedata\")\n",
    "        self.genomic_dir = os.path.join(self.genomedata_dir, \"genomic_fna\")\n",
    "        self.ex_dir = os.path.join(self.genomedata_dir, \"excluded_genomes\")\n",
    "        self.annotation_dir = os.path.join(self.genomedata_dir, \"annotations\")\n",
    "        self.fna_dir = os.path.join(self.genomedata_dir, \"fna_files\")\n",
    "        self.ffn_dir = os.path.join(self.genomedata_dir, \"ffn_files\")\n",
    "        self.gff_dir = os.path.join(self.genomedata_dir, \"gff_files\")\n",
    "        self.pangenome_dir = os.path.join(self.target_dir, \"pangenome\")\n",
    "        #self.results_dir = os.path.join(self.target_dir, \"results\")\n",
    "        self.coregene_dir = os.path.join(self.target_dir, \"coregenes\")\n",
    "        self.fasta_dir = os.path.join(self.coregene_dir, \"fasta\")\n",
    "        self.singlecopy = os.path.join(self.coregene_dir, \"singlecopy_genes.csv\")\n",
    "        self.ffn_seqs = os.path.join(self.coregene_dir, \"ffn_sequences.csv\")\n",
    "        self.alignments_dir = os.path.join(self.coregene_dir, \"alignments\")\n",
    "        self.consensus_dir = os.path.join(self.coregene_dir, \"consensus\")\n",
    "        self.blast_dir = os.path.join(self.coregene_dir, \"blast\")\n",
    "        self.primer_dir = os.path.join(self.target_dir, \"primer\")\n",
    "        self.primerblast_dir = os.path.join(self.primer_dir, \"primerblast\")\n",
    "        self.primer_qc_dir = os.path.join(self.primer_dir, \"primer_QC\")\n",
    "        self.mfold_dir = os.path.join(self.primer_dir, \"mfold\")\n",
    "        self.dimercheck_dir = os.path.join(self.primer_dir, \"dimercheck\")\n",
    "        self.summ_dir = os.path.join(self.config.path, \"Summary\", self.target)\n",
    "        self.contiglimit = 500\n",
    "        \n",
    "config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ags-bioinfo/blastdb/primerdesign'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#os.chdir('/home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conserved.part-1', 'conserved.part-0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['conserved.part-1', 'conserved.part-0']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dir = '/home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/'\n",
    "blast_dir = os.path.join(target_dir, \"coregenes\", \"blast\")\n",
    "def search_blastfiles(directory):\n",
    "    blast_files = [f for f in os.listdir(directory) if \".part-\" in f]\n",
    "    print(blast_files)\n",
    "    return blast_files\n",
    "\n",
    "search_blastfiles(blast_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import fnmatch\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "\n",
    "# paths\n",
    "pipe_dir = \"/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline\"\n",
    "dict_path = os.path.join(pipe_dir, \"dictionaries\")\n",
    "tmp_db_path = os.path.join(pipe_dir, 'tmp_config.json')\n",
    "if not pipe_dir in sys.path:\n",
    "    sys.path.append(pipe_dir)\n",
    "\n",
    "from scripts.configuration import errors\n",
    "from scripts.configuration import RunConfig\n",
    "from scripts.configuration import PipelineStatsCollector\n",
    "from basicfunctions import GeneralFunctions as G\n",
    "from basicfunctions import HelperFunctions as H\n",
    "from basicfunctions import ParallelFunctions as P\n",
    "from basicfunctions import BlastDBError\n",
    "\n",
    "\n",
    "class BlastPrep():\n",
    "    def __init__(self, directory, input_records, name, maxpartsize):\n",
    "        self.list_dict = {}\n",
    "        self.input_records = input_records\n",
    "        self.maxpartsize = maxpartsize\n",
    "        self.filename = name\n",
    "        self.directory = directory\n",
    "\n",
    "    def create_listdict(self):\n",
    "        groups = len(self.input_records) // self.maxpartsize\n",
    "        if len(self.input_records) % self.maxpartsize > 0:\n",
    "            groups = groups + 1\n",
    "        for i in range(0, groups):\n",
    "            if i not in self.list_dict.keys():\n",
    "                self.list_dict.update({i: []})\n",
    "\n",
    "    def get_equalgroups(self):\n",
    "        self.input_records.sort(key=lambda x: int(len(x.seq)), reverse=True)\n",
    "        list_start = 0\n",
    "        list_end = len(self.input_records) - 1\n",
    "        removed_key = []\n",
    "        key = 0\n",
    "        i = list_start\n",
    "        while key in self.list_dict.keys():\n",
    "            if key in removed_key:\n",
    "                key = key + 1\n",
    "            else:\n",
    "                item = self.input_records[i]\n",
    "                if len(self.list_dict[key]) < self.maxpartsize:\n",
    "                    self.list_dict[key].append(item)\n",
    "                    key = key + 1\n",
    "                    if key not in self.list_dict.keys():\n",
    "                        key = 0\n",
    "                    if i == list_end:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = i + 1\n",
    "                else:\n",
    "                    removed_key.append(key)\n",
    "\n",
    "    def write_blastinput(self):\n",
    "        for key in self.list_dict.keys():\n",
    "            if len(self.list_dict[key]) > 0:\n",
    "                file_name = os.path.join(\n",
    "                    self.directory, self.filename + \".part-\"+str(key))\n",
    "                SeqIO.write(self.list_dict[key], file_name, \"fasta\")\n",
    "\n",
    "\n",
    "    def run_blastprep(self):\n",
    "        G.logger(\"Run: run_blastprep - Preparing files for BLAST\")\n",
    "        print(\"\\nPreparing files for BLAST\")\n",
    "        self.create_listdict()\n",
    "        self.get_equalgroups()\n",
    "        cores = multiprocessing.cpu_count()\n",
    "        self.write_blastinput()\n",
    "        return cores\n",
    "\n",
    "\n",
    "class Blast(RunConfig):\n",
    "    def __init__(self, configuration, directory, mode):\n",
    "        print(\"Start BLAST\")\n",
    "        RunConfig.__init__(self, configuration)\n",
    "        self.directory = directory\n",
    "        self.mode = mode\n",
    "\n",
    "    def get_blast_cmd(self, blastfile, filename, cores):\n",
    "        fmt_file = os.path.join(dict_path, \"blastfmt6.csv\")\n",
    "        fmts = pd.read_csv(fmt_file, header=None).dropna()\n",
    "        blast_fmt = \" \".join([\"6\"] + list(fmts[0]))\n",
    "\n",
    "        if self.mode == \"quality_control\":\n",
    "            blast_cmd = [\n",
    "                \"blastn\", \"-task\", \"megablast\", \"-num_threads\",\n",
    "                str(cores), \"-query\", blastfile, \"-max_target_seqs\", \"5\",\n",
    "                \"-max_hsps\", \"1\", \"-out\", filename, \"-outfmt\", blast_fmt]\n",
    "\n",
    "        if self.mode == \"conserved\":\n",
    "            blast_cmd = [\n",
    "                \"blastn\", \"-task\", \"dc-megablast\", \"-num_threads\",\n",
    "                str(cores), \"-query\", blastfile, \"-max_target_seqs\",\n",
    "                \"2000\", \"-max_hsps\", \"1\", \"-out\", filename, \"-outfmt\", blast_fmt]\n",
    "\n",
    "        if self.mode == \"primer\":\n",
    "            blast_cmd = [\n",
    "                \"blastn\", \"-task\", \"blastn-short\", \"-num_threads\",\n",
    "                str(cores), \"-query\", blastfile,\n",
    "                \"-evalue\", \"500\", \"-out\", filename, \"-outfmt\", blast_fmt]\n",
    "\n",
    "        blast_cmd.append(\"-db\")\n",
    "        if self.config.customdb:\n",
    "            blast_cmd.append(self.config.customdb)\n",
    "        else:\n",
    "            blast_cmd.append(\"nt\")\n",
    "\n",
    "        return blast_cmd\n",
    "\n",
    "    def run_blast(self, name, use_cores):\n",
    "        G.logger(\"Run: run_blast - Start BLAST\")\n",
    "        blast_files = [f for f in os.listdir(directory) if \".part-\" in f]\n",
    "        blastfiles.sort(key=lambda x: int(x.split(\"part-\")[1]))\n",
    "        start = time.time()\n",
    "        os.chdir(self.directory)\n",
    "        for blastfile in blastfiles:\n",
    "            part = str(blastfile).split(\"-\")[1]\n",
    "            filename = name + \"_\" + part + \"_results.csv\"\n",
    "            results_path = os.path.join(self.directory, filename)\n",
    "            if self.mode == \"quality_control\":\n",
    "                blast_cmd = self.get_blast_cmd(\n",
    "                    blastfile, filename, use_cores)\n",
    "            elif not os.path.isfile(results_path):\n",
    "                blast_cmd = self.get_blast_cmd(\n",
    "                    blastfile, filename, use_cores)\n",
    "            else:\n",
    "                if os.stat(results_path).st_size == 0:\n",
    "                    blast_cmd = self.get_blast_cmd(\n",
    "                        blastfile, filename, use_cores)\n",
    "                else:\n",
    "                    blast_cmd = False\n",
    "                    G.comm_log(\"> Skip Blast step for \" + blastfile)\n",
    "            if blast_cmd:\n",
    "                try:\n",
    "                    G.run_subprocess(blast_cmd)\n",
    "                except (KeyboardInterrupt, SystemExit):\n",
    "                    G.keyexit_rollback(\n",
    "                        \"BLAST search\", dp=self.directory, fn=filename)\n",
    "                    raise\n",
    "\n",
    "        duration = time.time() - start\n",
    "        G.comm_log(\n",
    "            \"> Blast duration: \"\n",
    "            + str(timedelta(seconds=duration)).split(\".\")[0])\n",
    "        os.chdir(self.target_dir)\n",
    "\n",
    "\n",
    "class BlastParser(RunConfig):\n",
    "    def __init__(self, configuration, results=\"conserved\"):\n",
    "        RunConfig.__init__(self, configuration)\n",
    "        self.exception = configuration.exception\n",
    "        self.evalue = self.config.evalue\n",
    "        self.nuc_identity = self.config.nuc_identity\n",
    "        self.nontargetlist = configuration.nontargetlist\n",
    "        self.mode = results\n",
    "        self.start = time.time()\n",
    "        self.maxgroupsize = 25000\n",
    "        self.unlisted_spec = []\n",
    "    \n",
    "    def check_blastdb_errors(self, blastdf, filename):\n",
    "        if len(blastdf.index) == 0:\n",
    "            error_msg = \" \".join([\n",
    "                \"A problem with the BLAST results file\",\n",
    "                filename, \"was detected.\",\n",
    "                \"Please check if the file was removed and start the run again\"])\n",
    "\n",
    "        elif len(blastdf[blastdf[\"Subject Seq-id\"].str.contains(\"gnl|BL_ORD_ID|\", regex=False)]) > 0:\n",
    "            error_msg = (\n",
    "                \"Problem with custom DB, Please use the '-parse_seqids'\"\n",
    "                \" option for the makeblastdb command\")\n",
    "\n",
    "        elif len(blastdf[blastdf[\"Subject Title\"].str.contains(\"No definition line\", regex=False)]) > 0:\n",
    "            error_msg = (\n",
    "                \"Error: No definition line in Subject Title\"\n",
    "                \"\\nData is missing in the custom BLAST DB. At least \"\n",
    "                \"a unique sequence identifier and the species name \"\n",
    "                \"is required for each entry.\\nExpected format: \"\n",
    "                \">seqid species name optional description\")\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        logging.error(\"> \" + error_msg, exc_info=True)\n",
    "        errors.append([self.target, error_msg])\n",
    "        os.remove(filename)\n",
    "        print(\"removed \" + filename)\n",
    "        raise BlastDBError(error_msg)\n",
    "\n",
    "    def get_excluded_gis(self):\n",
    "        excluded_gis = []\n",
    "        gi_file = os.path.join(self.config_dir, \"no_blast.gi\")\n",
    "        if os.path.isfile(gi_file):\n",
    "            if os.stat(gi_file).st_size > 0:\n",
    "                with open(gi_file, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        gi = line.strip()\n",
    "                        excluded_gis.append(str(gi))\n",
    "        return excluded_gis\n",
    "        \n",
    "    def get_exceptions(self):\n",
    "        target_sp = \" \".join(\n",
    "            [\n",
    "                self.target.split(\"_\")[0], \n",
    "                H.subspecies_handler(self.target, mode=\"space\")])\n",
    "        exceptions = [target_sp]\n",
    "        if self.exception != []:\n",
    "            for item in self.exception:\n",
    "                exception = ' '.join(item.split(\"_\"))\n",
    "                if exception not in exceptions:\n",
    "                    exceptions.append(exception)\n",
    "        return exceptions\n",
    "\n",
    "    def get_species_names_from_title(self, df):\n",
    "        if self.config.virus is True:\n",
    "            df.loc[:, \"Species\"] = df.loc[:, \"Subject Title\"].str.split(\",\").str[0]\n",
    "        else:\n",
    "            subsp_filter = df[\"Subject Title\"].str.contains(\"|\".join([\"subsp.\", \"pv.\"]))\n",
    "            df.loc[\n",
    "                subsp_filter, \"Species\"\n",
    "                ] = df.loc[\n",
    "                            subsp_filter, \"Subject Title\"\n",
    "                                ].str.split(\" \").str[0:4].apply(\n",
    "                                            lambda x: ' '.join(x))\n",
    "            df.loc[\n",
    "                ~subsp_filter, \"Species\"\n",
    "                ] = df.loc[\n",
    "                            ~subsp_filter, \"Subject Title\"\n",
    "                                ].str.split(\" \").str[0:2].apply(\n",
    "                                            lambda x: ' '.join(x))\n",
    "        return df\n",
    "\n",
    "    def quality_control(self, blastdf, exceptions):\n",
    "        blastdf = blastdf.sort_values(\n",
    "            [\"Query Seq-id\", \"Bit score\"], ascending=False)\n",
    "        blastdf = blastdf.drop_duplicates([\"Query Seq-id\"])\n",
    "        mask = blastdf[\"Species\"].str.contains(\"|\".join(exceptions))\n",
    "        blastdf.loc[mask, \"QC status\"] = \"passed QC\"\n",
    "        blastdf.loc[~mask, \"QC status\"] = \"failed QC\"\n",
    "        blastdf[\"Target species\"] = exceptions[0]\n",
    "        QC_results = blastdf[[\n",
    "                \"Query Seq-id\", \"Subject GI\", \"Subject accession\",\n",
    "                \"Species\", \"Target species\", \"QC status\"]]\n",
    "        return QC_results\n",
    "\n",
    "    def offtarget_sequences(self, offtarget_hits):\n",
    "        if self.config.nuc_identity > 0:\n",
    "            offtarget_hits = offtarget_hits[\n",
    "                offtarget_hits[\n",
    "                    'Percentage of identical matches'] >= self.config.nuc_identity]\n",
    "\n",
    "        offtarget_hits = offtarget_hits[\n",
    "                offtarget_hits['Expect value'] <= self.config.evalue]\n",
    "\n",
    "        partialseqs = self.check_seq_ends(offtarget_hits)\n",
    "\n",
    "        offtarget_summary = offtarget_hits[[\n",
    "            \"Query Seq-id\", \"Species\", \"Subject GI\", \"Subject accession\",\n",
    "            'Percentage of identical matches', 'Expect value', 'Bit score']]\n",
    "        return offtarget_summary, partialseqs        \n",
    "    \n",
    "    \n",
    "    def parse_blastrecords(self, blastdf, excluded_gis, exceptions):\n",
    "        # remove excluded sequences from the results\n",
    "        blastdf = blastdf[~blastdf[\"Subject GI\"].isin(excluded_gis)]\n",
    "        blastdf = blastdf[~blastdf[\"Subject accession\"].isin(excluded_gis)]\n",
    "        # Extract species names from title\n",
    "        blastdf = self.get_species_names_from_title(blastdf)\n",
    "        if self.mode == \"quality_control\":\n",
    "            QC_results = self.quality_control(blastdf, exceptions)\n",
    "            partialseqs = pd.DataFrame()\n",
    "            return QC_results, partialseqs\n",
    "\n",
    "        offtarget_hits = blastdf[~blastdf[\"Species\"].isin(exceptions)]\n",
    "        if self.config.nolist is False:\n",
    "            offtarget_filter = (offtarget_hits[\"Species\"].str.contains(\"|\".join(self.nontargetlist)))\n",
    "            notonlist = offtarget_hits[~offtarget_filter]\n",
    "            offtarget_hits = offtarget_hits[offtarget_filter]\n",
    "            self.unlisted_spec.extend(notonlist[\"Species\"].unique())\n",
    "            \n",
    "        if self.mode == \"conserved\":\n",
    "            offtarget_summary, partialseqs = self.offtarget_sequences(offtarget_hits)\n",
    "            \n",
    "        if self.mode == \"primer\":\n",
    "            partialseqs = pd.DataFrame()\n",
    "            offtarget_summary = offtarget_hits[[\n",
    "                \"Query Seq-id\", \"Species\", \"Subject GI\", \"Subject accession\", \n",
    "                \"Start of alignment in subject\", \"End of alignment in subject\", \n",
    "                \"Subject sequence length\", 'Percentage of identical matches', 'Expect value', 'Bit score']]\n",
    "        \n",
    "        return offtarget_summary, partialseqs\n",
    "            \n",
    "    def check_seq_ends(self, offtarget):\n",
    "        # Filter non-aligned endings\n",
    "        offtarget.loc[:, 'overhang'] = (\n",
    "                            offtarget.loc[\n",
    "                                :,'Query sequence length'\n",
    "                            ] - offtarget.loc[:, 'End of alignment in query'])\n",
    "  \n",
    "        partials_max = offtarget.sort_values(\n",
    "            'overhang', ascending=True).drop_duplicates(['Query Seq-id'])\n",
    "        keep_max = partials_max[partials_max['overhang'] >= self.config.minsize]\n",
    "\n",
    "        partials_min = offtarget.sort_values(\n",
    "                'Start of alignment in query', ascending=True\n",
    "            ).drop_duplicates(['Query Seq-id'])\n",
    "        keep_min = partials_min[\n",
    "                partials_min[\n",
    "                    \"Start of alignment in query\"] >= self.config.minsize]\n",
    "        keep_min = keep_min.assign(Start=1)\n",
    "\n",
    "        # Write sequence range data\n",
    "        mindata = keep_min[['Query Seq-id', \"Start\", \"Start of alignment in query\"]]\n",
    "        maxdata = keep_max[['Query Seq-id', 'End of alignment in query', 'Query sequence length']]\n",
    "        mindata.columns = [\"ID\", \"Start\", \"Stop\"]\n",
    "        maxdata.columns = [\"ID\", \"Start\", \"Stop\"]\n",
    "        \n",
    "        partial_seqs = pd.concat([mindata, maxdata], sort=False)\n",
    "\n",
    "        return partial_seqs\n",
    "\n",
    "    def create_blastdf(self, filename, header):\n",
    "        try:\n",
    "            blastdf = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "            blastdf.columns = header\n",
    "            blastdf = blastdf.astype(\n",
    "                {\"Subject GI\": str, \"Subject accession\": str})\n",
    "        except pd.errors.EmptyDataError:\n",
    "            blastdf = pd.DataFrame()    \n",
    "        self.check_blastdb_errors(blastdf, filename)\n",
    "        return blastdf\n",
    "    \n",
    "    def combine_blast_results(self, blast_dir):\n",
    "        offtarget_dfs = []\n",
    "        partial_dfs = []     \n",
    "        blastresults = [\n",
    "            os.path.join(blast_dir, f) for f in os.listdir(blast_dir) \n",
    "            if f.endswith(\"results.csv\")]\n",
    "        blastresults.sort()\n",
    "\n",
    "        exceptions = self.get_exceptions()\n",
    "        excluded_gis = self.get_excluded_gis()\n",
    "        print(\"Excluded GI(s):\", excluded_gis)\n",
    "        \n",
    "        fmt_file = os.path.join(dict_path, \"blastfmt6.csv\")\n",
    "        header = list(pd.read_csv(fmt_file, header=None)[1].dropna())\n",
    "        \n",
    "        print(\"open BLAST result files\")\n",
    "        for i, filename in enumerate(blastresults):\n",
    "            print(str(i+1) + \"/\" + str(len(blastresults)))\n",
    "\n",
    "            blastdf = self.create_blastdf(filename, header)\n",
    "            offtarget, partialseqs = self.parse_blastrecords(\n",
    "                blastdf, excluded_gis, exceptions)\n",
    "\n",
    "            offtarget_dfs.append(offtarget)\n",
    "            partial_dfs.append(partialseqs)\n",
    "\n",
    "        offtarget = pd.concat(offtarget_dfs)\n",
    "        partial = pd.concat(partial_dfs)\n",
    "        \n",
    "        return offtarget, partial\n",
    "\n",
    "    def write_mostcommonhits(self, df):\n",
    "        to_file = os.path.join(self.blast_dir, \"mostcommonhits.csv\")\n",
    "        total = len(df.index)\n",
    "        queries = len(set(df[\"Query Seq-id\"]))\n",
    "        mostcommon = pd.DataFrame(df.drop_duplicates([\"Query Seq-id\", \"Species\"])[\"Species\"].value_counts())\n",
    "        mostcommon.index.name =\"Species\"\n",
    "        mostcommon.columns = [\"BLAST hits [count]\"]\n",
    "        mostcommon[\"BLAST hits [% of queries]\"] = mostcommon[\"BLAST hits [count]\"].apply(lambda x: round(100/queries*x, 1))\n",
    "        mostcommon.sort_values(\"BLAST hits [% of queries]\", ascending=False, inplace=True)\n",
    "        f_head = str(\"Total BLAST hits,Number of queries\\n\" + str(total) + \",\" + str(queries) + \"\\n\")\n",
    "        with open(to_file, \"w\") as f:\n",
    "            f.write(f_head)\n",
    "        mostcommon.to_csv(to_file, mode='a')\n",
    "    \n",
    "    \n",
    "    def write_primer3_input(self, offtarget_seqs):\n",
    "        G.create_directory(self.primer_dir)\n",
    "        file_path = os.path.join(self.primer_dir, \"primer3_input\")\n",
    "        controlfile_path = os.path.join(self.coregene_dir, \".primer3_input\")\n",
    "        conserved_seqs = os.path.join(self.blast_dir, \"conserved_seqs.fas\")\n",
    "        conserved_seq_dict = SeqIO.to_dict(SeqIO.parse(conserved_seqs, \"fasta\"))\n",
    "        conserved = list(set(conserved_seq_dict.keys()) - set(offtarget_seqs))\n",
    "        selected_recs = [conserved_seq_dict[k] for k in conserved]\n",
    "        \n",
    "        if self.config.probe is True:\n",
    "            probe = \"\\nPRIMER_PICK_INTERNAL_OLIGO=1\"\n",
    "        else:\n",
    "            probe = \"\"\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            for rec in selected_recs:\n",
    "                f.write(\n",
    "                        \"SEQUENCE_ID=\" + rec.id + \"\\nSEQUENCE_TEMPLATE=\"\n",
    "                        + str(rec.seq)\n",
    "                        + \"\\nPRIMER_PRODUCT_SIZE_RANGE=\"\n",
    "                        + str(self.config.minsize) + \"-\"\n",
    "                        + str(self.config.maxsize) + probe + \"\\n=\\n\")\n",
    "\n",
    "            partial_file = os.path.join(self.blast_dir, \"partialseqs.csv\")\n",
    "            if os.path.isfile(partial_file):\n",
    "                parts = pd.read_csv(partial_file, header=None)\n",
    "                seq_id = parts[0].to_list()\n",
    "                start = parts[1].to_list()\n",
    "                end = parts[2].to_list()\n",
    "                for i, idx in enumerate(seq_id):\n",
    "                    f.write(\n",
    "                        \"SEQUENCE_ID=\" + idx + \"\\nSEQUENCE_TEMPLATE=\"\n",
    "                        + str(conserved_seq_dict[idx].seq)[start[i]:end[i]]\n",
    "                        + \"\\nPRIMER_PRODUCT_SIZE_RANGE=\"\n",
    "                        + str(self.config.minsize) + \"-\"\n",
    "                        + str(self.config.maxsize) + probe + \"\\n=\\n\")\n",
    "        self.changed_primer3_input(file_path, controlfile_path)\n",
    "\n",
    "    def find_differences_in_p3input(self):\n",
    "        new = []\n",
    "        old = []\n",
    "        with open(file_path) as n:\n",
    "            for line in n:\n",
    "                if \"SEQUENCE_ID=\" in line:\n",
    "                    if line.strip() not in new:\n",
    "                        new.append(line.strip())\n",
    "                if \"PRIMER_PICK_INTERNAL_OLIGO=\" in line:\n",
    "                    if line.strip() not in new:\n",
    "                        new.append(line.strip())\n",
    "\n",
    "        with open(controlfile_path) as o:\n",
    "            for line in o:\n",
    "                if \"SEQUENCE_ID=\" in line:\n",
    "                    if line.strip() not in old:\n",
    "                        old.append(line.strip())\n",
    "                if \"PRIMER_PICK_INTERNAL_OLIGO=\" in line:\n",
    "                    if line.strip() not in old:\n",
    "                        old.append(line.strip())\n",
    "\n",
    "        diff = list(set(new) ^ set(old))\n",
    "        return diff    \n",
    "    \n",
    "    def changed_primer3_input(self, file_path, controlfile_path):\n",
    "        if os.path.isfile(controlfile_path):\n",
    "            diff = self.find_differences_in_p3input()\n",
    "            if len(diff) > 0:\n",
    "                info1 = (\n",
    "                    \"Due to changed settings primer design \"\n",
    "                    \"and quality control will start from scratch\")\n",
    "                info2 = \"Differences in primer3 input:\"\n",
    "                for info in [info1, info2, diff]:\n",
    "                    G.comm_log(info)\n",
    "                if os.path.isdir(self.primer_dir):\n",
    "                    G.comm_log(\"Delete primer directory\")\n",
    "                    shutil.rmtree(self.primer_dir)\n",
    "                G.create_directory(self.primer_dir)\n",
    "                shutil.copy(file_path, controlfile_path)\n",
    "        else:\n",
    "            shutil.copy(file_path, controlfile_path)\n",
    "    \n",
    "    def interpret_blastresults(self, blast_dir, offtarget, partial):\n",
    "        if self.mode == \"quality_control\":\n",
    "            if offtarget.empty:\n",
    "                G.comm_log(\"> No Quality Control results found\")\n",
    "                errors.append([self.target, error_msg])\n",
    "            else:\n",
    "                qc_dir = os.path.basename(blast_dir)\n",
    "                fp = os.path.join(blast_dir, qc_dir + \"_report.csv\")\n",
    "                offtarget.to_csv(fp, index=False)\n",
    "\n",
    "        if partial.empty is False:\n",
    "            part_file = os.path.join(blast_dir, \"partialseqs.csv\")\n",
    "            partial.to_csv(part_file, index=False, header=False)\n",
    "\n",
    "        fp = os.path.join(blast_dir, \"offtarget_summary.csv\")\n",
    "        offtarget.to_csv(fp, index=False)    \n",
    "\n",
    "        if self.mode == \"conserved\":\n",
    "            offtarget_seqs = offtarget['Query Seq-id'].unique()\n",
    "            self.write_primer3_input(offtarget_seqs)\n",
    "            self.write_mostcommonhits(offtarget)\n",
    "            return\n",
    "            \n",
    "        db_seqs = self.find_primerbinding_offtarget_seqs(offtarget)\n",
    "        self.get_primerBLAST_DBIDS(db_seqs)\n",
    "        self.write_nontarget_sequences()\n",
    "        \n",
    "    def run_blastparser(self):\n",
    "        if self.mode == \"primer\":\n",
    "            print(\"Start primer blast parser\")\n",
    "            offtarget = self.bp_parse_results(self.primerblast_dir)\n",
    "            db_seqs = self.find_primerbinding_offtarget_seqs(offtarget)\n",
    "            self.get_primerBLAST_DBIDS(db_seqs)\n",
    "            self.write_nontarget_sequences()\n",
    "\n",
    "            duration = time.time() - self.start\n",
    "            G.comm_log(\n",
    "                \"> Primer blast parser time: \"\n",
    "                + str(timedelta(seconds=duration)).split(\".\")[0])\n",
    "        else:\n",
    "            specific_ids = self.bp_parse_results(self.blast_dir)\n",
    "            self.write_primer3_input(specific_ids)\n",
    "            duration = time.time() - self.start\n",
    "            msg = (\"species specific conserved sequences: \"\n",
    "                    + str(len(specific_ids)))\n",
    "            G.comm_log(\n",
    "                \"> Blast parser time: \"\n",
    "                + str(timedelta(seconds=duration)).split(\".\")[0])\n",
    "            print(timedelta(seconds=duration))\n",
    "            G.comm_log(msg)\n",
    "            PipelineStatsCollector(self.config).write_stat(msg)\n",
    "\n",
    "            if len(specific_ids) == 0:\n",
    "                msg = \"> No conserved sequences without non-target match found\"\n",
    "                G.comm_log(msg)\n",
    "                errors.append([self.target, msg])\n",
    "                return 1\n",
    "\n",
    "            return 0\n",
    "\n",
    "    def main(self):\n",
    "        qc_gene = \"rRNA\"\n",
    "        #qc_dir = os.path.join(self.genomedata_dir, qc_gene + \"_QC\")\n",
    "        blast_dir = self.primerblast_dir\n",
    "        offtarget, partial = self.combine_blast_results(blast_dir)\n",
    "        self.interpret_blastresults(blast_dir, offtarget, partial)\n",
    "\n",
    "    def find_primerbinding_offtarget_seqs(self, df):\n",
    "        df.loc[:, \"Primer pair\"] = df.loc[:, \"Query Seq-id\"].str.split(\"_\").str[0:-1].apply(\n",
    "                                                                        lambda x: '_'.join(x))\n",
    "        df.sort_values(['Start of alignment in subject'], inplace=True)\n",
    "        \n",
    "        testfile = os.path.join(self.primer_dir, \"all_seqs.csv\")\n",
    "        df.to_csv(testfile)\n",
    "\n",
    "        fwd_df = df[df[\"Query Seq-id\"].str.endswith(\"_F\")]\n",
    "        rev_df =  df[df[\"Query Seq-id\"].str.endswith(\"_R\")]\n",
    "        int_df = pd.merge(\n",
    "                    fwd_df, rev_df, how ='inner',\n",
    "                    on =['Subject accession', 'Primer pair'], suffixes=(\"_F\", \"_R\"))\n",
    "\n",
    "        testfile = os.path.join(self.primer_dir, \"merged_seqs.csv\")\n",
    "        int_df.to_csv(testfile)\n",
    "        \n",
    "        f = int_df[[\n",
    "            'Subject accession', 'Start of alignment in subject_F',\n",
    "            'End of alignment in subject_F', 'Subject sequence length_F']]\n",
    "        r = int_df[[\n",
    "            'Subject accession', 'Start of alignment in subject_R',\n",
    "            'End of alignment in subject_R', 'Subject sequence length_R']]\n",
    "        std_cols = [\n",
    "            'Subject accession', 'Start of alignment in subject',\n",
    "            'End of alignment in subject', 'Subject sequence length']\n",
    "\n",
    "        f.columns, r.columns = std_cols, std_cols\n",
    "        common = pd.concat([f, r], sort=False)\n",
    "        common.reset_index(drop=True, inplace=True)\n",
    "        return common\n",
    "\n",
    "    def get_primerBLAST_DBIDS(self, offtarget):\n",
    "        print(\"\\nGet sequence accessions of BLAST hits\\n\")\n",
    "        G.logger(\"> Get sequence accessions of BLAST hits\")\n",
    "        G.create_directory(self.primer_qc_dir)\n",
    "        overhang=2000\n",
    "        output_path = os.path.join(self.primer_qc_dir, \"primerBLAST_DBIDS.csv\")\n",
    "        if os.path.isfile(output_path):\n",
    "            return 0\n",
    "        # data manipulation\n",
    "        strandfilter = offtarget['Start of alignment in subject'] > offtarget['End of alignment in subject']\n",
    "        offtarget.loc[strandfilter, \"Start overhang\"] = offtarget.loc[strandfilter, 'End of alignment in subject'] - overhang\n",
    "        offtarget.loc[~strandfilter, \"Start overhang\"] = offtarget.loc[~strandfilter,'Start of alignment in subject'] - overhang\n",
    "        offtarget.loc[strandfilter, \"End overhang\"] = offtarget.loc[strandfilter, 'End of alignment in subject'] + overhang\n",
    "        offtarget.loc[~strandfilter, \"End overhang\"] = offtarget.loc[~strandfilter,'Start of alignment in subject'] + overhang\n",
    "\n",
    "        overfilter = offtarget[\"End overhang\"] > offtarget['Subject sequence length']\n",
    "        offtarget.loc[overfilter, \"End overhang\"] = offtarget.loc[overfilter, 'Subject sequence length']\n",
    "        lowfilter = offtarget[\"Start overhang\"] < 1\n",
    "        offtarget.loc[lowfilter, \"Start overhang\"] = 1\n",
    "        # datatype to int\n",
    "        offtarget[\"Start overhang\"] = offtarget[\"Start overhang\"].astype('Int64')\n",
    "        offtarget[\"End overhang\"] = offtarget[\"End overhang\"].astype('Int64')\n",
    "\n",
    "        # data binning\n",
    "        max_range = offtarget[\"End overhang\"].max()\n",
    "        stepsize = self.config.maxsize + overhang*2 + 1\n",
    "        collection = []\n",
    "        for i in range(1, max_range,  stepsize):\n",
    "            j = i + overhang*2 + self.config.maxsize\n",
    "            sub = offtarget[offtarget[\"Start overhang\"].between(i, j, inclusive=True)]\n",
    "            mini = sub.groupby([\"Subject accession\"])[\"Start overhang\"].min()\n",
    "            maxi = sub.groupby([\"Subject accession\"])[\"End overhang\"].max()\n",
    "            submax = pd.concat([mini, maxi], axis=1)\n",
    "            submax.columns = [\"Start\", \"Stop\"]\n",
    "            submax.sort_values([\"Start\", \"Stop\"], inplace=True, ascending=False)\n",
    "            submax.drop_duplicates(inplace=True)\n",
    "            collection.append(submax)\n",
    "\n",
    "        results = pd.concat(collection)\n",
    "\n",
    "        if len(results.index) == 0:\n",
    "            msg = (\n",
    "                \"Error did not find any sequences for non-target DB. \"\n",
    "                + \"Please check the species list and/or BLAST database\")\n",
    "            print(msg)\n",
    "            G.logger(\"> \" + msg)\n",
    "            errors.append([self.target, msg])\n",
    "            return 1\n",
    "\n",
    "        results.index.name = \"accession\"\n",
    "        results.to_csv(output_path, header=None)\n",
    "        return 0\n",
    "\n",
    "    def write_sequences(self, fasta_seqs, range_dict, filename):\n",
    "        recs = []\n",
    "        for item in fasta_seqs:\n",
    "            fulldesc = item[0]\n",
    "            desc = \" \".join(fulldesc.split(\" \")[1:])\n",
    "            acc = fulldesc.split(\".\")[0][1::]\n",
    "            acc_desc = fulldesc.split(\":\")[0][1::]\n",
    "            seqrange = fulldesc.split(\":\")[1].split(\" \")[0].split(\"-\")\n",
    "            fullseq = \"\".join(item[1::])\n",
    "            for start, stop in range_dict[acc]:\n",
    "                desc_range = str(int(seqrange[0]) + start) + \"_\" + str(int(seqrange[0]) + stop)\n",
    "                acc_id = acc_desc + \"_\" + desc_range\n",
    "                seq = fullseq[start:stop]\n",
    "                rec = SeqRecord(Seq(seq), id=acc_id, description=desc)\n",
    "                recs.append(rec)\n",
    "\n",
    "        SeqIO.write(recs, filename, \"fasta\")\n",
    "\n",
    "    def write_nontarget_sequences(self):\n",
    "        # faster but requires more RAM\n",
    "        db = self.config.customdb\n",
    "        if db is None:\n",
    "            db = \"nt\"\n",
    "\n",
    "        dbids = os.path.join(self.primer_qc_dir, \"primerBLAST_DBIDS.csv\")\n",
    "        df = pd.read_csv(dbids, header=None)\n",
    "        df.columns = [\"Accession\", \"Start\", \"Stop\"]\n",
    "        df.sort_values([\"Accession\"], inplace=True)\n",
    "\n",
    "        seqcount = len(df.index)\n",
    "        G.comm_log(\"Found \" + str(seqcount) + \" sequences for the non-target DB\", newline=True)\n",
    "        parts = len(df.index)//self.maxgroupsize + 1\n",
    "        chunks = np.array_split(df, parts)\n",
    "\n",
    "        for part, chunk in enumerate(chunks):\n",
    "            start = chunk.groupby([\"Accession\"])[\"Start\"].min()\n",
    "            stop = chunk.groupby([\"Accession\"])[\"Stop\"].max()\n",
    "            one_extraction = pd.concat([start, stop], axis=1).reset_index().values.tolist()\n",
    "\n",
    "            keys = list(set(chunk[\"Accession\"]))\n",
    "            range_dict = {}\n",
    "            for k in keys:\n",
    "                ranges = (chunk[chunk[\"Accession\"] == k][[\"Start\", \"Stop\"]].values - start[k]).tolist()\n",
    "                range_dict.update({k: ranges})\n",
    "\n",
    "            filename = \"BLASTnontarget\" + str(part) + \".sequences\"\n",
    "            filepath = os.path.join(self.primer_qc_dir, filename)\n",
    "            if not os.path.isfile(filepath):\n",
    "                G.comm_log(\"Start writing \" + filename)\n",
    "                G.comm_log(\"Start DB extraction\")\n",
    "                fasta_seqs = G.run_parallel(\n",
    "                        P.get_seq_fromDB, one_extraction, db)\n",
    "                try:\n",
    "                    self.write_sequences(fasta_seqs, range_dict, filepath)\n",
    "                except (KeyboardInterrupt, SystemExit):\n",
    "                    G.keyexit_rollback(\"DB extraction\", fp=filepath)\n",
    "                    raise\n",
    "                G.comm_log(\"Finished writing \" + filename)\n",
    "            else:\n",
    "                G.comm_log(\"Skip writing \" + filename)\n",
    "        \n",
    "        \n",
    "BlastParser(config, \"primer\").main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Found pangenome directory, skip QC \n",
      "SpeciesPrimer run finished for Lactobacillus_curvatus \n",
      " End: Wed Jul 21 16:06:59 2021 \n",
      " See results in /home/ags-bioinfo/blastdb/primerdesign/test/Summary/Lactobacillus_curvatus\n",
      "Run: run_pangenome_analysis(Lactobacillus_curvatus)\n",
      "> Pangenome directory already exists\n",
      "Continue with existing Pangenome data\n",
      "> Collect results of pan-genome analysis\n",
      "core genes: 717\n",
      "single copy core genes: 669\n",
      "> Start alignment of core gene sequences\n",
      "progress 0 % [                                                  ]\n",
      "\n",
      "> Find consensus sequence for aligned core gene sequences\n",
      "progress 0 % [                                                  ]\n",
      "\n",
      "> Search conserved regions in consensus sequences\n",
      "> Number of conserved sequences: 1661\n",
      "Run: blastresults_files(Lactobacillus_curvatus)\n",
      "Excluded GI(s): []\n",
      "\n",
      "open BLAST result file 1/2\n",
      "\n",
      "open BLAST result file 2/2\n",
      "0:00:10.691485\n",
      "> Start primer design\n",
      "Run primer3_core -p3_settings_file=/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline/dictionaries/p3parameters -output=/home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/primer/primer3_output /home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/primer/primer3_input\n",
      "> potential primer pair(s): 423\n",
      "\n",
      "Preparing files for BLAST\n",
      "Start BLAST\n",
      "Run blastn -task blastn-short -num_threads 12 -query primer.part-0 -evalue 500 -out primer_0_results.csv -outfmt 6 qseqid qlen qstart qend sseqid stitle sgi sacc sstart send slen evalue bitscore length pident nident mismatch gapopen staxids -db /home/ags-bioinfo/blastdb/BLASTDB_nt/ref_prok_rep_genomes\n",
      "Start primer blast parser\n",
      "Run: blastresults_files(Lactobacillus_curvatus)\n",
      "Excluded GI(s): []\n",
      "\n",
      "open BLAST result file 1/1\n",
      "\n",
      "Get sequence accessions of BLAST hits\n",
      "\n",
      "\n",
      "Found 116 sequences for the non-target DB\n",
      "\n",
      "Start writing BLASTnontarget0.sequences\n",
      "Start DB extraction\n",
      "progress 100 % [**************************************************]\n",
      "\n",
      "Finished writing BLASTnontarget0.sequences\n",
      "0:00:13.829436\n",
      "\n",
      "Start index template.sequences\n",
      "Start index BLASTnontarget0.sequences\n",
      "\n",
      "Start index genomic.sequencesRun IndexDB.py /home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/primer/primer_QC/BLASTnontarget0.sequences -k 9\n",
      "\n",
      "\n",
      "Run IndexDB.py /home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/primer/primer_QC/genomic.sequences -k 9Run IndexDB.py /home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/primer/primer_QC/template.sequences -k 9\n",
      "\n",
      "Done indexing template.sequences\n",
      "Done indexing BLASTnontarget0.sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-21:\n",
      "ERROR:root:KeyboardInterrupt during DB indexing\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline/basicfunctions.py\", line 607, in index_database\n",
      "    cmd, True, True, False)\n",
      "  File \"/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline/basicfunctions.py\", line 57, in run_subprocess\n",
      "    check_output()\n",
      "  File \"/home/ags-bioinfo/blastdb/primerdesign/speciesprimer/pipeline/basicfunctions.py\", line 49, in check_output\n",
      "    output = process.stdout.readline().decode().strip()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remove genomic.sequences.sqlite3.db genomic.sequences.2bit genomic.sequences genomic.sequences.uni genomic.sequences.unifasta\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ags-bioinfo/anaconda3/envs/speciesprimerdeps/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ags-bioinfo/anaconda3/envs/speciesprimerdeps/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ags-bioinfo/anaconda3/envs/speciesprimerdeps/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n"
     ]
    }
   ],
   "source": [
    "from scripts.qualitycontrol import QualityControl\n",
    "from scripts.coregenes import PangenomeAnalysis\n",
    "from scripts.coregenes import CoreGenes\n",
    "from scripts.coregenes import CoreGeneSequences\n",
    "from scripts.primerdesign import PrimerDesign\n",
    "from scripts.primerdesign import PrimerQualityControl\n",
    "from scripts.summary import Summary\n",
    "\n",
    "qc_data = QualityControl(config).quality_control()\n",
    "Summary(config).run_summary()\n",
    "exitstat = PangenomeAnalysis(config).run_pangenome_analysis()\n",
    "CoreGenes(config).run_CoreGenes()\n",
    "CoreGeneSequences(config).run_coregeneanalysis()\n",
    "PrimerDesign(config).run_primerdesign()\n",
    "PrimerQualityControl(config).run_primer_qc()\n",
    "Summary(config).run_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rRNA_QC'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/ags-bioinfo/blastdb/primerdesign/test/Lactobacillus_curvatus/genomedata/rRNA_QC\"\n",
    "os.path.basename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
